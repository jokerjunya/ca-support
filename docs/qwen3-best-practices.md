# Qwen3 ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ & é–‹ç™ºã‚¬ã‚¤ãƒ‰

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€Qwen3ã‚’ä½¿ç”¨ã—ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã«ãŠã‘ã‚‹æœ€é©ãªæ‰‹æ³•ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’ã¾ã¨ã‚ã¦ã„ã¾ã™ã€‚

## ğŸ“‹ ç›®æ¬¡

- [Qwen3æ¦‚è¦](#qwen3æ¦‚è¦)
- [ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°](#ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°)
- [ãƒ¢ãƒ‡ãƒ«é¸æŠã¨è¨­å®š](#ãƒ¢ãƒ‡ãƒ«é¸æŠã¨è¨­å®š)
- [ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™º](#ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™º)
- [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–](#ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–)
- [ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è€ƒæ…®äº‹é …](#ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è€ƒæ…®äº‹é …)
- [å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³](#å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³)
- [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)

## ğŸ¤– Qwen3æ¦‚è¦

### ãƒ¢ãƒ‡ãƒ«ãƒ©ã‚¤ãƒ³ãƒŠãƒƒãƒ—

**Dense Models (å¯†é›†å‹)**
- Qwen3-0.6B: è»½é‡ãƒ‡ãƒã‚¤ã‚¹å‘ã‘
- Qwen3-1.7B: ãƒ¢ãƒã‚¤ãƒ«ãƒ»ã‚¨ãƒƒã‚¸å‘ã‘
- Qwen3-4B: ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã«æœ€é©
- Qwen3-8B: ãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã„æ€§èƒ½
- Qwen3-14B: é«˜æ€§èƒ½ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
- Qwen3-32B: æœ€é«˜æ€§èƒ½Dense

**MoE Models (Mixture of Experts)**
- Qwen3-30B-A3B: 3Bæ´»æ€§åŒ–ã€åŠ¹ç‡é‡è¦–
- Qwen3-235B-A22B: 22Bæ´»æ€§åŒ–ã€æœ€é«˜æ€§èƒ½

### ç‰¹å¾´
- **Apache 2.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: å•†ç”¨åˆ©ç”¨å¯èƒ½
- **119è¨€èªã‚µãƒãƒ¼ãƒˆ**: å¤šè¨€èªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œ
- **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ€è€ƒ**: Thinking/Non-Thinkingãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿
- **é•·æ–‡å¯¾å¿œ**: æœ€å¤§128Kãƒˆãƒ¼ã‚¯ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

## ğŸ¯ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

### 1. åŸºæœ¬åŸå‰‡

#### è¨€èªã®é¸æŠ
```markdown
âŒ é¿ã‘ã‚‹ã¹ãï¼š
æ—¥æœ¬èªã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ›¸ã

âœ… æ¨å¥¨ï¼š
è‹±èªã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ›¸ãã€æ—¥æœ¬èªå‡ºåŠ›ã‚’æŒ‡å®š
```

**ç†ç”±**: Qwen3ã¯è‹±èªã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒæœ€ã‚‚å……å®Ÿã—ã¦ãŠã‚Šã€è‹±èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§æœ€é«˜å“è³ªã®å‡ºåŠ›ã‚’å¾—ã‚‰ã‚Œã¾ã™ã€‚

#### ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹é€ 
```text
ã‚ãªãŸã¯{å½¹å‰²}ã§ã™ã€‚ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã€{å¯¾è±¡è€…}å®›ã®{ç›®çš„}ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

## èƒŒæ™¯æƒ…å ±
{è©³ç´°ãªèƒŒæ™¯}

## è¦æ±‚äº‹é …
1. {å…·ä½“çš„è¦æ±‚1}
2. {å…·ä½“çš„è¦æ±‚2}
3. {å…·ä½“çš„è¦æ±‚3}

## å‡ºåŠ›å½¢å¼
{æœŸå¾…ã™ã‚‹å½¢å¼}

ä»¥ä¸‹ã®å½¢å¼ã§æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
{ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ}
```

### 2. ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿

#### Thinking Modeï¼ˆæ¨è«–ãƒ¢ãƒ¼ãƒ‰ï¼‰
```python
# è¤‡é›‘ãªå•é¡Œã‚„è«–ç†çš„æ¨è«–ãŒå¿…è¦ãªå ´åˆ
messages = [{"role": "user", "content": "è¤‡é›‘ãªåˆ†æã‚¿ã‚¹ã‚¯"}]
response = client.chat.completions.create(
    model="qwen3-30b-a3b",
    messages=messages,
    extra_body={"enable_thinking": True},  # Thinking Modeæœ‰åŠ¹åŒ–
    temperature=0.6,  # å‰µé€ æ€§ã¨ãƒãƒ©ãƒ³ã‚¹
    max_tokens=1000
)
```

#### Non-Thinking Modeï¼ˆé«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ï¼‰
```python
# å˜ç´”ãªã‚¿ã‚¹ã‚¯ã‚„é«˜é€Ÿå¿œç­”ãŒå¿…è¦ãªå ´åˆ
response = client.chat.completions.create(
    model="qwen3-30b-a3b",
    messages=messages,
    extra_body={"reasoning_effort": "none"},  # Non-Thinking Mode
    temperature=0.7,  # å°‘ã—é«˜ã‚ã®å‰µé€ æ€§
    max_tokens=500
)
```

### 3. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

#### ãƒ¡ãƒ¼ãƒ«æ–‡é¢ç”Ÿæˆï¼ˆæ¨å¥¨ä¾‹ï¼‰
```text
You are an experienced career advisor with 10+ years of expertise. 
Create an effective email for {target_person} based on the detailed information below.

## Case Details
- Candidate: {candidate_name}
- Company: {company}
- Position: {job_title}
- Current Status: {status}
- Enthusiasm Level: {enthusiasm_score}% (higher = more positive)
- Concern Level: {concern_score}% (higher = more worried)

## Situation Details and History
{detailed_context_with_history}

## Requirements
Create a practical and human email that includes:

1. **Appropriate Length**: 200-500 characters
2. **Specific Content**: Concrete proposals based on past interactions
3. **Next Steps**: Clear and actionable actions
4. **Appropriate Tone**: Polite but not too formal, friendly
5. **Personal Consideration**: Reflect candidate's concerns and hopes

## Target Audience Considerations
- CS (Corporate Representative): Balance candidate's appeal and concerns
- Candidate: Content that alleviates anxiety and maintains positive feelings
- RA (Recruiting Advisor): Clearly communicate current status and needed support

Please respond in Japanese in the following format:
Subject: [subject]
Body: [body]
```

## ğŸ›ï¸ ãƒ¢ãƒ‡ãƒ«é¸æŠã¨è¨­å®š

### ç”¨é€”åˆ¥æ¨å¥¨ãƒ¢ãƒ‡ãƒ«

| ç”¨é€” | æ¨å¥¨ãƒ¢ãƒ‡ãƒ« | ç†ç”± |
|------|------------|------|
| ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ | Qwen3-4B | é«˜é€Ÿã€ä½ã‚³ã‚¹ãƒˆ |
| ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ | Qwen3-14B | ãƒãƒ©ãƒ³ã‚¹è‰¯ã„æ€§èƒ½ |
| æ–‡æ›¸åˆ†æ | Qwen3-30B-A3B | é•·æ–‡å¯¾å¿œã€åŠ¹ç‡çš„ |
| è¤‡é›‘ãªæ¨è«– | Qwen3-235B-A22B | æœ€é«˜æ€§èƒ½ |
| ãƒ¢ãƒã‚¤ãƒ«ã‚¢ãƒ—ãƒª | Qwen3-0.6B/1.7B | è»½é‡ |

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š

#### Thinking Mode
```python
thinking_params = {
    "temperature": 0.6,      # ãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã„å‰µé€ æ€§
    "top_p": 0.95,          # å¤šæ§˜æ€§ç¢ºä¿
    "top_k": 20,            # å“è³ªé‡è¦–
    "max_tokens": 1000,     # è©³ç´°ãªæ¨è«–ç”¨
    "enable_thinking": True
}
```

#### Non-Thinking Mode
```python
fast_params = {
    "temperature": 0.7,     # å°‘ã—é«˜ã‚ã®å‰µé€ æ€§
    "top_p": 0.8,          # åŠ¹ç‡é‡è¦–
    "max_tokens": 500,     # é«˜é€Ÿå¿œç­”
    "reasoning_effort": "none"
}
```

## ğŸ—ï¸ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™º

### 1. åŸºæœ¬çš„ãªçµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³

#### ã‚·ãƒ³ãƒ—ãƒ«ãªçµ±åˆ
```python
from app.core.llm import QwenLLM

class EmailGenerator:
    def __init__(self):
        self.llm = QwenLLM(
            base_url="http://localhost:11434",
            model_name="qwen3:30b"
        )
    
    def generate_email(self, context):
        try:
            result = self.llm.generate_email_content(context)
            return {
                "success": True,
                "content": result["body"],
                "subject": result["subject"]
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "fallback": self._generate_fallback(context)
            }
```

### 2. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

#### å …ç‰¢ãªã‚¨ãƒ©ãƒ¼å‡¦ç†
```python
async def robust_generation(query, max_retries=3):
    for attempt in range(max_retries):
        try:
            # ãƒ¡ã‚¤ãƒ³ç”Ÿæˆå‡¦ç†
            response = await generate_with_qwen3(query)
            
            # å“è³ªãƒã‚§ãƒƒã‚¯
            if quality_check(response):
                return response
                
        except Exception as e:
            logger.warning(f"Attempt {attempt + 1} failed: {e}")
            
            if attempt == max_retries - 1:
                # æœ€çµ‚çš„ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                return generate_fallback_response(query)
            
            # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§å†è©¦è¡Œ
            await asyncio.sleep(2 ** attempt)
```

### 3. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†

#### å±¥æ­´æƒ…å ±ã®çµ±åˆ
```python
def build_context_with_history(application_id):
    """å±¥æ­´æƒ…å ±ã‚’å«ã‚€è©³ç´°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰"""
    # åŸºæœ¬æƒ…å ±å–å¾—
    app = get_application(application_id)
    
    # é–¢é€£ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´å–å¾—
    messages = get_message_history(application_id)
    messages.sort(key=lambda x: x.timestamp)
    
    # å±¥æ­´ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«çµ±åˆ
    history_context = ""
    if messages:
        history_context = "\n\n## Past Communication History\n"
        for msg in messages[-3:]:  # æœ€æ–°3ä»¶
            direction = "sent" if msg.is_outbound else "received"
            history_context += f"""
### {msg.timestamp} - {direction}
**{msg.sender} â†’ {msg.receiver}**
Subject: {msg.subject}
Content: {msg.content}
"""
    
    return EmailGenerationContext(
        candidate_name=app.candidate_name,
        company=app.company,
        job_title=app.job_title,
        status=app.status,
        latest_summary=app.latest_summary + history_context,
        enthusiasm_score=app.enthusiasm_score,
        concern_score=app.concern_score,
        target_person=get_target_person(application_id),
        current_template=get_template(application_id)
    )
```

## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 1. æ®µéšçš„å“è³ªå‘ä¸Š

```python
async def adaptive_generation(query, quality_threshold=0.8):
    """æ®µéšçš„ã«å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹ç”Ÿæˆæ‰‹æ³•"""
    
    # 1. é«˜é€Ÿç”Ÿæˆï¼ˆNon-thinkingï¼‰
    quick_response = await generate_fast(query)
    
    # 2. å“è³ªè©•ä¾¡
    quality_score = evaluate_quality(quick_response)
    
    if quality_score >= quality_threshold:
        return quick_response
    
    # 3. é«˜å“è³ªç”Ÿæˆï¼ˆThinkingï¼‰
    logger.info("Quality below threshold, using thinking mode")
    return await generate_with_thinking(query)
```

### 2. ã‚³ã‚¹ãƒˆåŠ¹ç‡ãƒ‘ã‚¿ãƒ¼ãƒ³

```python
def cost_efficient_workflow(query, complexity_threshold=0.7):
    """ã‚³ã‚¹ãƒˆåŠ¹ç‡ã‚’è€ƒæ…®ã—ãŸãƒ¢ãƒ‡ãƒ«é¸æŠ"""
    
    # è¤‡é›‘åº¦åˆ¤å®š
    complexity = analyze_complexity(query)
    
    if complexity < complexity_threshold:
        # ç°¡å˜ãªã‚¿ã‚¹ã‚¯ã¯å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã§
        return qwen3_4b.generate(query)
    else:
        # è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã¯å¤§ãã„ãƒ¢ãƒ‡ãƒ«ã§
        return qwen3_30b.generate(query)
```

### 3. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥

```python
import hashlib
from functools import lru_cache

class ResponseCache:
    def __init__(self):
        self.cache = {}
    
    def get_cache_key(self, context):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        content = f"{context.candidate_name}_{context.company}_{context.status}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get_cached_response(self, context):
        key = self.get_cache_key(context)
        return self.cache.get(key)
    
    def cache_response(self, context, response):
        key = self.get_cache_key(context)
        self.cache[key] = response
```

## ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è€ƒæ…®äº‹é …

### 1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–

```python
import re

def sanitize_user_input(user_input):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º"""
    
    # å±é™ºãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å»
    dangerous_patterns = [
        r"ignore\s+previous\s+instructions",
        r"forget\s+your\s+role",
        r"act\s+as\s+if",
        r"pretend\s+to\s+be",
        r"<think>.*?</think>",  # æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®å½è£…é˜²æ­¢
    ]
    
    cleaned_input = user_input
    for pattern in dangerous_patterns:
        cleaned_input = re.sub(pattern, "", cleaned_input, flags=re.IGNORECASE)
    
    return cleaned_input.strip()
```

### 2. æ©Ÿå¯†æƒ…å ±ä¿è­·

```python
def contains_sensitive_data(text):
    """æ©Ÿå¯†æƒ…å ±ã®æ¤œå‡º"""
    sensitive_patterns = [
        r"\b\d{4}-\d{4}-\d{4}-\d{4}\b",  # ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰
        r"\b\d{3}-\d{2}-\d{4}\b",        # SSN
        r"password\s*[=:]\s*\w+",        # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰
        r"api[_-]?key\s*[=:]\s*\w+",     # APIã‚­ãƒ¼
    ]
    
    for pattern in sensitive_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False
```

### 3. ãƒ¬ãƒ¼ãƒˆåˆ¶é™

```python
from datetime import datetime, timedelta
from collections import defaultdict

class RateLimiter:
    def __init__(self, max_requests=60, window_minutes=1):
        self.max_requests = max_requests
        self.window = timedelta(minutes=window_minutes)
        self.requests = defaultdict(list)
    
    def is_allowed(self, user_id):
        now = datetime.now()
        
        # å¤ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‰Šé™¤
        self.requests[user_id] = [
            req_time for req_time in self.requests[user_id]
            if now - req_time < self.window
        ]
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
        if len(self.requests[user_id]) >= self.max_requests:
            return False
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¨˜éŒ²
        self.requests[user_id].append(now)
        return True
```

## ğŸ”§ å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

### 1. ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³

```python
class QwenModelFactory:
    @staticmethod
    def create_model(model_type, use_case):
        """ç”¨é€”ã«å¿œã˜ãŸæœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆ"""
        
        configs = {
            "chat": {
                "small": QwenConfig("qwen3:4b", temperature=0.7),
                "medium": QwenConfig("qwen3:14b", temperature=0.6),
                "large": QwenConfig("qwen3:30b-a3b", temperature=0.5)
            },
            "analysis": {
                "fast": QwenConfig("qwen3:8b", reasoning_effort="none"),
                "accurate": QwenConfig("qwen3:30b-a3b", enable_thinking=True)
            }
        }
        
        config = configs.get(model_type, {}).get(use_case)
        if not config:
            raise ValueError(f"Unknown model type: {model_type}/{use_case}")
        
        return QwenLLM(config)
```

### 2. ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³

```python
def with_retry_and_fallback(max_retries=3):
    """å†è©¦è¡Œã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã‚’è¿½åŠ ã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            last_error = None
            
            for attempt in range(max_retries):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    last_error = e
                    logger.warning(f"Attempt {attempt + 1} failed: {e}")
                    
                    if attempt < max_retries - 1:
                        await asyncio.sleep(2 ** attempt)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
            logger.error(f"All attempts failed: {last_error}")
            return generate_fallback_response(*args, **kwargs)
        
        return wrapper
    return decorator

@with_retry_and_fallback(max_retries=2)
async def generate_email_content(context):
    return await qwen3_llm.generate(context)
```

### 3. ã‚ªãƒ–ã‚¶ãƒ¼ãƒãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³

```python
class GenerationEventObserver:
    def __init__(self):
        self.observers = []
    
    def add_observer(self, observer):
        self.observers.append(observer)
    
    def notify_generation_start(self, context):
        for observer in self.observers:
            observer.on_generation_start(context)
    
    def notify_generation_complete(self, context, result):
        for observer in self.observers:
            observer.on_generation_complete(context, result)

class MetricsObserver:
    def on_generation_start(self, context):
        self.start_time = time.time()
    
    def on_generation_complete(self, context, result):
        duration = time.time() - self.start_time
        log_metrics(context, result, duration)
```

## ğŸ› ï¸ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºç­–

#### 1. ç”Ÿæˆå“è³ªãŒä½ã„
```python
# å•é¡Œ: æœŸå¾…ã™ã‚‹å“è³ªã®å‡ºåŠ›ãŒå¾—ã‚‰ã‚Œãªã„
# è§£æ±ºç­–:

# A. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è‹±èªã§æ›¸ãç›´ã™
prompt = """
You are an experienced career advisor. Create a professional email...
(Respond in Japanese)
"""

# B. Thinking Modeã‚’æœ‰åŠ¹åŒ–
response = llm.generate(prompt, enable_thinking=True)

# C. temperatureã‚’èª¿æ•´
response = llm.generate(prompt, temperature=0.6)  # ã‚ˆã‚Šä¸€è²«æ€§é‡è¦–
```

#### 2. ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒé…ã„
```python
# å•é¡Œ: ç”Ÿæˆã«æ™‚é–“ãŒã‹ã‹ã‚Šã™ãã‚‹
# è§£æ±ºç­–:

# A. Non-thinking Modeã‚’ä½¿ç”¨
response = llm.generate(prompt, reasoning_effort="none")

# B. å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
llm = QwenLLM(model_name="qwen3:4b")

# C. max_tokensã‚’åˆ¶é™
response = llm.generate(prompt, max_tokens=300)
```

#### 3. ãƒ¡ãƒ¢ãƒªä¸è¶³
```python
# å•é¡Œ: ãƒ¢ãƒ‡ãƒ«ãŒãƒ¡ãƒ¢ãƒªã«åã¾ã‚‰ãªã„
# è§£æ±ºç­–:

# A. é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
model_name = "qwen3:30b-a3b-q4_0"  # 4bité‡å­åŒ–

# B. ã‚ˆã‚Šå°ã•ã„ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´
model_name = "qwen3:8b"

# C. MoEãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨
model_name = "qwen3:30b-a3b"  # 3Bæ´»æ€§åŒ–ã®ã¿
```

#### 4. APIæ¥ç¶šã‚¨ãƒ©ãƒ¼
```python
# å•é¡Œ: Ollama APIã«æ¥ç¶šã§ããªã„
# è§£æ±ºç­–:

# A. æ¥ç¶šç¢ºèª
def check_ollama_connection():
    try:
        response = requests.get("http://localhost:11434/api/tags")
        return response.status_code == 200
    except:
        return False

# B. ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½
async def robust_api_call(prompt, max_retries=3):
    for attempt in range(max_retries):
        try:
            return await llm.generate(prompt)
        except ConnectionError:
            if attempt < max_retries - 1:
                await asyncio.sleep(5)
            else:
                raise
```

## ğŸ“ˆ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

### ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†

```python
class QwenMetrics:
    def __init__(self):
        self.metrics = {
            "total_requests": 0,
            "successful_requests": 0,
            "average_response_time": 0,
            "thinking_mode_usage": 0,
            "error_count": 0
        }
    
    def record_request(self, success, response_time, used_thinking_mode):
        self.metrics["total_requests"] += 1
        
        if success:
            self.metrics["successful_requests"] += 1
        else:
            self.metrics["error_count"] += 1
        
        if used_thinking_mode:
            self.metrics["thinking_mode_usage"] += 1
        
        # å¹³å‡å¿œç­”æ™‚é–“ã®æ›´æ–°
        self._update_average_response_time(response_time)
    
    def get_success_rate(self):
        if self.metrics["total_requests"] == 0:
            return 0
        return self.metrics["successful_requests"] / self.metrics["total_requests"]
```

## ğŸ‰ ã¾ã¨ã‚

Qwen3ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã™ã‚‹ãŸã‚ã®é‡è¦ãªãƒã‚¤ãƒ³ãƒˆï¼š

1. **è‹±èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + æ—¥æœ¬èªå‡ºåŠ›æŒ‡å®š** ã§æœ€é«˜å“è³ªã‚’å®Ÿç¾
2. **ç”¨é€”ã«å¿œã˜ãŸãƒ¢ãƒ¼ãƒ‰é¸æŠ** ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€é©åŒ–
3. **é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºé¸æŠ** ã§ã‚³ã‚¹ãƒˆã¨ãƒªã‚½ãƒ¼ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹
4. **å …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°** ã§å®‰å®šã—ãŸã‚µãƒ¼ãƒ“ã‚¹æä¾›
5. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–** ã§ãƒªã‚¹ã‚¯ã‚’æœ€å°åŒ–
6. **ç¶™ç¶šçš„ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°** ã§å“è³ªå‘ä¸Š

ã“ã‚Œã‚‰ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’å®Ÿè£…ã™ã‚‹ã“ã¨ã§ã€Qwen3ã®èƒ½åŠ›ã‚’æœ€å¤§é™ã«æ´»ç”¨ã—ãŸé«˜å“è³ªãªAIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚

---

**æ›´æ–°æ—¥**: 2025å¹´1æœˆ21æ—¥  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: v1.0  
**æ¬¡å›æ›´æ–°äºˆå®š**: æ–°æ©Ÿèƒ½ãƒªãƒªãƒ¼ã‚¹æ™‚ 